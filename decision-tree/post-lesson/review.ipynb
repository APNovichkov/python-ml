{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Decision Trees - yet another classifier model\n",
    "\n",
    "**Decision trees can be described by the following:**\n",
    "- Considered to be one of the most mature and traditional algorithms in predictive analytics\n",
    "- Used to solve classification problems through visual and explicit representations of decisions and decision making\n",
    "- Essentially a map where you can follow the path according to your decisions at every step, and at the end you get your prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why and when do we need Decision Trees?\n",
    "\n",
    "- When features are catgorical: when we can classify data into known groups\n",
    "- When we want to model a set of sequential, heirarchical decisions that lead to some final result\n",
    "- When we need to explain (to a boss) the reasoning behind a specific decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the components of Decision Trees?\n",
    "**Here is what makes up a decision tree:**\n",
    "- Root Node\n",
    "- Leaf Node\n",
    "\n",
    "We can obtain these root and leaf nodes via:\n",
    "- Conditional probability\n",
    "- Entropy\n",
    "- Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The steps and defenitions behind building a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "Entropy is a measure of uncertainty for a random variable. The higher the entropy, the more uncertain we are of the variable value at a certain event\n",
    "\n",
    "$H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "Where the coin has two outcomes, for example: p(H) = .5 and p(T) = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of the coin with probability of .5 has the most uncertainty when we flip it, so the entropy is at the max of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "p = [.9, .1]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that an unfair coin has a way lower entropy, meaning we are more certain of the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "Conditional probability can be described as the answer to this question:  \n",
    ">Given that some friends like Chocolate, what is the probability that they like Strawberry as well?\n",
    "\n",
    "The formula that we can use to find that probability is this one:\n",
    "  \n",
    "  \n",
    "$ P( Strawberry \\mid Chocolate ) = \\frac{P( Chocolate \\cap Strawberry )}{P( Chocolate )} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tennis.csv', delimiter='\\t', names=['1', 'Outlook', 'Temp', 'Humidity', 'Wind', 'Decision_to_play'])\n",
    "df = df.drop(['1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75, 0.25)\n",
      "(0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "def obtain_conditional_prob(input_df, col, condition, target):\n",
    "    \"\"\"\n",
    "    Obtain conditional probability of decision being yes or no given a df column and a condition\n",
    "    Return two probabilities in a tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    df = input_df\n",
    "    if condition != 'Target':\n",
    "        df = input_df[input_df[col] == condition]\n",
    "    df = df[[col, target]]\n",
    "    \n",
    "    total = len(df)\n",
    "    pos = df[target].value_counts()[0]\n",
    "    neg = total - pos\n",
    "    \n",
    "    return (pos/total, neg/total)\n",
    "\n",
    "# what are the probabilities of Decision_to_play given Wind is Weak?\n",
    "print(obtain_conditional_prob(df,'Wind', 'Weak', 'Decision_to_play'))\n",
    "\n",
    "# what are the probabilities of Decision_to_play given Wind is Strong?\n",
    "print(obtain_conditional_prob(df, 'Wind', 'Strong', 'Decision_to_play'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "Information gain measures how much information a feature gives us about the decision (output class)  \n",
    "> This is the main feature used by a Decision Tree algorithm to construct a Decision Tree\n",
    "\n",
    "Decision Trees will always try to maximize information gain and the higher the information gain that a feature has, the more likely it is to be at the top of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the formula for calculating Information Gain for a specific feature:**  \n",
    "  \n",
    "$I(Decision; Wind) = H(Decision) - \\sum P(Wind) * H(Decision | Wind)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_feature_prob(input_df, col, condition):\n",
    "    \"\"\"\n",
    "    Return probability of a certain condition being true in a single column(feature)\n",
    "    \"\"\"\n",
    "    total = len(input_df[col])\n",
    "    val = len(input_df[input_df[col] == condition])\n",
    "    return val/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(p_list):\n",
    "    \"\"\" \n",
    "    Returns entropy for a tuple of probabilities. \n",
    "    This represents uncertainty of the tuple as a whole. The bigger the entropy, the more uncertainty\n",
    "    Formula = -[P(H)*log^2(H) + P(T)*log^2(T)]\n",
    "    \"\"\"\n",
    "    num1 = p_list[0]\n",
    "    num2 = p_list[1]\n",
    "    \n",
    "    # Edge Case\n",
    "    if num1 == 0 or num2 == 0:\n",
    "        return 0\n",
    "\n",
    "    return -(num1*np.log2(num1) + num2*np.log2(num2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_gain(df, col, decision):\n",
    "    \"\"\"\n",
    "    Calculate and return the info gain for a specific feature\n",
    "    \"\"\"\n",
    "    \n",
    "    conditions = df[col].value_counts().keys()\n",
    "    \n",
    "    # Obtain the entropy of the decision column itself\n",
    "    H_decision = get_entropy(obtain_conditional_prob(df, col, 'Target', decision))\n",
    "    \n",
    "    # Calculate the summation part of the formula\n",
    "    p_s = []\n",
    "    for condition in conditions:\n",
    "        val = obtain_feature_prob(df, col, condition) * get_entropy(obtain_conditional_prob(df, col, condition, decision))\n",
    "        p_s.append(val)\n",
    "        \n",
    "    return H_decision - (np.sum(p_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info gain between Outlook and Decision is 0.24674981977443933\n",
      "info gain between Temp and Decision is 0.02922256565895487\n",
      "info gain between Humidity and Decision is 0.15183550136234159\n",
      "info gain between Wind and Decision is 0.04812703040826949\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns[:-1]:\n",
    "    info_gain = get_info_gain(df, col, df.columns[-1:][0])\n",
    "    print(f'info gain between {col} and Decision is {info_gain}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can see that the Info Gain for the Outlook feature was the highest, so we should use it as the root of the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now branch out from the Outlook feature, and build sub-decision trees\n",
    "We stop when we reach either a set depth, or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
