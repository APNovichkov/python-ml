{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Model Evaluation\n",
    "\n",
    "This topic covers things like:\n",
    "- Confusion Matrix\n",
    "- True/False Positives/Negatives\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Error\n",
    "- Specificity\n",
    "- Precision\n",
    "- F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with the Confusion Matrix\n",
    "This is a way to quantify the performance of a classification model from which we can then obtain such things as accuracy, specificity, precision, f1-score and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion Matrix looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted     0   1\n",
    "# Actual:    0 TN  FP\n",
    "# Actual:    1 FN  TP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we have two axis: predicted(what classification model predicts) and the actual values. Notice that in this case we only have two predictors: 0 and 1, so our confusion matrix is 2x2.\n",
    "\n",
    "**Q for teacher:** Can classification models predict more than 2 states/and if so will we have an nxn confusion matrix, where n is the # of states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out by using logistic regression on a diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "feature_df = df[feature_cols]\n",
    "\n",
    "X = feature_df.to_numpy()\n",
    "y = df['Outcome'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate our own confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(actual_list, predicted_list):\n",
    "    out = [[0., 0.],[0., 0.]]\n",
    "\n",
    "    assert(len(actual_list) == len(predicted_list))\n",
    "    \n",
    "    for actual,predicted in zip(actual_list, predicted_list):\n",
    "        # True Negatives/positives\n",
    "        if actual == predicted: # True {something}\n",
    "            if actual == 0: # True Negatices\n",
    "                out[0][0] += 1\n",
    "            else: # True Positives\n",
    "                out[1][1] += 1\n",
    "        else: # False {something}\n",
    "            if predicted == 1: # False Positives\n",
    "                out[0][1] += 1\n",
    "            else: # False negatives\n",
    "                out[1][0] += 1\n",
    "    \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114.,  16.],\n",
       "       [ 46.,  16.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use an sklearn module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114,  16],\n",
       "       [ 46,  16]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "c_matrix = confusion_matrix(y_test, y_pred)\n",
    "c_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this tell us?**\n",
    "Well, we can get 4 values from this:\n",
    "- True Negatives - [0][0] - Predicted was 0 and so was actual\n",
    "- True Positives - [1][1] - Predicted was 1 and so was actual\n",
    "- False Negatives - [1][0] - Predicted was 0 but actual was 1 (These are very bad!)\n",
    "- False Positives - [0][1] - Predicted was 1 but actual was 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these values we can calculate all sorts of things, let's go through them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "**Overall, how often is our classifier correct**  \n",
    "> TruePositives + TrueNegatives / sum(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6770833333333334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(confusion_matrix):\n",
    "    total = confusion_matrix.sum().sum()\n",
    "    tp_plus_tn = confusion_matrix[1][1] + confusion_matrix[0][0]\n",
    "    return tp_plus_tn/total\n",
    "\n",
    "get_accuracy(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error\n",
    "**Overall, how often is our classifier incorrect**  \n",
    "> FalsePositives + FalseNegatives / sum(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3229166666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_error(confusion_matrix):\n",
    "    total = confusion_matrix.sum().sum()\n",
    "    fp_plus_fn = confusion_matrix[0][1] + confusion_matrix[1][0]\n",
    "    return fp_plus_fn/total\n",
    "\n",
    "get_error(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall (Want it to be very low!)\n",
    "**When the actual value is positive, how often is the prediction correct**  \n",
    "> TruePositives / TruePositive + FalseNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25806451612903225"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recall(confusion_matrix):\n",
    "    tp_plus_tn = confusion_matrix[1][1] + confusion_matrix[1][0]\n",
    "    tp = confusion_matrix[1][1]\n",
    "    return tp/tp_plus_tn\n",
    "\n",
    "get_recall(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "**When a positive value is predicted, how often is the prediction correct**  \n",
    "> TruePositives / TruePositives + FalsePositives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_precision(confusion_matrix):\n",
    "    tp_plus_fp = confusion_matrix[1][1] + confusion_matrix[0][1]\n",
    "    tp = confusion_matrix[1][1]\n",
    "    return tp/tp_plus_fp\n",
    "\n",
    "get_precision(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity\n",
    "**When the actual value is negative, how often is the prediction correct**  \n",
    "> TrueNegatives / TrueNegatives + FalsePositives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8769230769230769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_specificity(confusion_matrix):\n",
    "    tn_plus_fp = confusion_matrix[0][0] + confusion_matrix[0][1]\n",
    "    tn = confusion_matrix[0][0]\n",
    "    return tn/tn_plus_fp\n",
    "\n",
    "get_specificity(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "**What we can use to evaluate some classification models in some cases**  \n",
    "> 2 * (Precision * Recall) / (Precision + Recall)  \n",
    "\n",
    "The higher the F1 score, the better. But if Recall is 0, we can get a 0 F1-score, but the model can still be good. Have to take it case by case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3404255319148936"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_f1_score(confusion_matrix):\n",
    "    numerator = 2*get_precision(confusion_matrix) * get_recall(confusion_matrix)\n",
    "    denominator = get_precision(confusion_matrix) + get_recall(confusion_matrix)\n",
    "    return numerator/denominator\n",
    "\n",
    "get_f1_score(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also evaluate models using sklearn\n",
    "We can use k-means cross validation to obtain values like recall/f1-score for our logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is k-means cross validation? \n",
    "It is a technique used to validate models by splitting the input data into k # of parts, and one by one making 1-1/k of that data the training data, and the other part the test data(X and y respectively). This gives an output of k f1-scores or recalls(or whatever other metric we want), and we can take the mean and get the average performance of our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing logistic regression, we can also apply weights to the outcome classes. For example in the case below, we have more 0 outcomes than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome value counts: \n",
      "0    500\n",
      "1    268\n",
      "Name: Outcome, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "feature_df = df[feature_cols]\n",
    "\n",
    "X = feature_df.to_numpy()\n",
    "y = df['Outcome']\n",
    "\n",
    "print(f'Outcome value counts: \\n{y.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, here is how we can instantiate our logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(class_weight={1: 500/268}, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: figure out what the '1' is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then, right away, get some metric by using cross_val_score function of of that logistic regression model with sklearn.  \n",
    "In this case we are going to be using **5-means** cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Accuracies: [0.64935065 0.65584416 0.64935065 0.70588235 0.65359477]\n",
      "Mean Accuracy: 0.6628045157456922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "all_accuracies = cross_val_score(estimator=log_reg, X=X, y=y, cv=5, scoring='accuracy')\n",
    "mean_accuracy = all_accuracies.mean()\n",
    "\n",
    "print(f'All Accuracies: {all_accuracies}')\n",
    "print(f'Mean Accuracy: {mean_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All f1 scores: [0.578125   0.55462185 0.54237288 0.64       0.576     ]\n",
      "Mean f1 score: 0.5782239460190857\n"
     ]
    }
   ],
   "source": [
    "all_f1_scores = cross_val_score(estimator=log_reg, X=X, y=y, cv=5, scoring='f1')\n",
    "mean_f1_score = all_f1_scores.mean()\n",
    "\n",
    "print(f'All f1 scores: {all_f1_scores}')\n",
    "print(f'Mean f1 score: {mean_f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that the f1 score here is higher than what we got before. This is because we **did not** assign a weight to our logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for the future:\n",
    "When faced with choosing which classification model to do. Let's say between logistic regression and SVM, we should choose the one with the least variance in the cross validation score, but if both are very similar, choose the one that gives the highest accuracy mean!  \n",
    "\n",
    "Also, when doing k-means cross validation, k is usually chosen as 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best possible hyper-parameters for SVM machine(polynomial/rbf)\n",
    "We can use what is called as grid-search that chooses the best C and gamma parameters for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='linear'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to find the best C to use for using an SVM with the same diabetes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_selection(X,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
